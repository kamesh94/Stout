{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "df = pd.read_csv(r'C:/Users/kames/OneDrive/Documents/GitHub/Stout/loans_full_schema.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization 1\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "locationDf=df.groupby(['state'], as_index=False)[['interest_rate']].mean()\n",
    "\n",
    "locFig = go.Figure(data=go.Choropleth(\n",
    "    locations=locationDf['state'],\n",
    "    z = locationDf['interest_rate'].astype(float),\n",
    "    locationmode = 'USA-states',\n",
    "    colorscale = 'Reds',\n",
    "    colorbar_title = \"Average Interest Rate\",\n",
    "))\n",
    "\n",
    "locFig.update_layout(\n",
    "    title_text = 'Heatmap of state-wise average interest rate',\n",
    "    geo_scope='usa',\n",
    ")\n",
    "\n",
    "locFig.show()\n",
    "\n",
    "locFig.write_html('docs/data-visualization-1.1.html')\n",
    "\n",
    "densityDf=df.groupby(['state'],as_index=False).mean()[['state', 'debt_to_income', 'interest_rate']]\n",
    "\n",
    "denfig = go.Figure(data=go.Choropleth(\n",
    "    locations=densityDf['state'],\n",
    "    z = densityDf['debt_to_income'].astype(float),\n",
    "    locationmode = 'USA-states',\n",
    "    colorscale = 'Blues',\n",
    "    colorbar_title = \"Average Debt-To-Income Ratio\",\n",
    "))\n",
    "\n",
    "denfig.update_layout(\n",
    "    title_text = 'Heatmap of state-wise debt-to-income ratio',\n",
    "    geo_scope='usa',\n",
    ")\n",
    "\n",
    "denfig.show()\n",
    "\n",
    "denfig.write_html('docs/data-visualization-1.2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization 2\n",
    "\n",
    "#Preparing Delinquencies and how does it affect interest rate\n",
    "\n",
    "delinqDf = df[['delinq_2y', 'months_since_last_delinq','account_never_delinq_percent', 'interest_rate']]\n",
    "df['months_since_last_delinq_categories'] = pd.cut(x=df['months_since_last_delinq'], bins=[0, 20, 40, 60, 80, 100, 120],\n",
    "                     labels=['0-20', '20-40', '40-60','60-80','80-100','100-120'])\n",
    "df['account_never_delinq_percent_categories'] = pd.cut(x=df['account_never_delinq_percent'], bins=[0, 20, 40, 60, 80, 100],\n",
    "                     labels=['0-20', '20-40', '40-60','60-80','80-100'])\n",
    "\n",
    "figdelinqDf = px.box(df, x=\"delinq_2y\", y=\"interest_rate\")\n",
    "figdelinqDf.show()\n",
    "figdelinqDf.write_html('docs/data-visualization-2.1.html')\n",
    "\n",
    "figLastDel = px.box(df, x=\"months_since_last_delinq_categories\", y=\"interest_rate\")\n",
    "figLastDel.show()\n",
    "figLastDel.write_html('docs/data-visualization-2.2.html')\n",
    "\n",
    "figAcctNotDel = px.box(df, x=\"account_never_delinq_percent_categories\", y=\"interest_rate\")\n",
    "figAcctNotDel.show()\n",
    "figAcctNotDel.write_html('docs/data-visualization-2.3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data visualization 3\n",
    "\n",
    "grouping = df.groupby(['issue_month', 'grade'], as_index=False).agg(loan_amount = ('loan_amount', 'mean'), interest_rate = ('interest_rate','mean'))\n",
    "grouping = grouping.sort_values(by=['issue_month'])\n",
    "figLoan = px.scatter(grouping, x=\"loan_amount\", y=\"interest_rate\", animation_frame=\"issue_month\", animation_group=\"grade\",\n",
    "           color=\"grade\",\n",
    "           log_x=True, size_max=55, range_x=[100,100000], range_y=[0,30])\n",
    "\n",
    "figLoan.show()\n",
    "\n",
    "figLoan.write_html('docs/data-visualization-3.1.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature set selection and predicting interest_rate\n",
    "\n",
    "#Step 1: Data exploration\n",
    "\n",
    "#We have 10,000 observations on 55 variables.\n",
    "\n",
    "#1.a We evaluate the missing values columns using the below function\n",
    "\n",
    "def missing_zero_values_table(df):\n",
    "        zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)\n",
    "        mz_table = mz_table.rename(\n",
    "        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})\n",
    "        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n",
    "        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)\n",
    "        mz_table['Data Type'] = df.dtypes\n",
    "        mz_table = mz_table[\n",
    "            mz_table.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "            \"There are \" + str(mz_table.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
    "        return mz_table\n",
    "missing_zero_values_table(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the above function output we have 11 columns with 0/NAN values. We now evaluate whether these missing values make sense and can they be ignored or imputed\n",
    "\n",
    "#For num_accounts_120d_past_due column, we have 9682 0 values and 318 missing values. We can impute 0 to the missing values here easily\n",
    "df.num_accounts_120d_past_due = df.num_accounts_120d_past_due.fillna(0)\n",
    "\n",
    "#For months_since_last_delinq and months_since_90d_late, there are no 0 values. One can assume that they have no such history and impute 0\n",
    "df.months_since_last_delinq = df.months_since_last_delinq.fillna(0)\n",
    "df.months_since_90d_late = df.months_since_90d_late.fillna(0)\n",
    "\n",
    "#Missing Emp length may be replaced with the median to maintain sanctity of data\n",
    "df.emp_length = df.emp_length.fillna(df.emp_length.median())\n",
    "\n",
    "#emp_title, emp_length, debt_to_income and months_since_last_credit_inquiry have low percent of 0/NAN values hence we ignore them for now.\n",
    "\n",
    "#verification_income_joint, annual_income_joint and debt_to_income_joint have the most missing values each almost 85%.\n",
    "#These missing  values make sense since they will only be populated when application_type = joint.\n",
    "countOfJointApplications = df[df.application_type=='joint'].shape[0] #1495\n",
    "percentOfJointApplication = (countOfJointApplications/df.shape[0])*100 #14.95\n",
    "#From the above calculation, it is very clear that the number of missing 'joint' type values (mostly 85%) is due to joint applications (14.95%)\n",
    "\n",
    "#We impute 'Not Applicable' value for verification_income_joint column\n",
    "df['verification_income_joint'] = df.verification_income_joint.fillna('Not Applicable')\n",
    "\n",
    "#It is hard to impute the joint income and debt ratio. We cannot consider joint income to be 0 since that would lead to extremely high value from debt ratio\n",
    "#One way of dealing with this is to split the dataset into two datasets based on application type and model interest rate for each\n",
    "#But that does not give us a good model. Since this data is MNAR - Missing Not At Random, we will evaluate the outliers for these columns before imputing the value\n",
    "\n",
    "px.box(x=df.annual_income_joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.b Outlier detection and handling\n",
    "\n",
    "#Our analysis above for missing values led to analyze the outliers first before go back and judge the best way to handle those missing values\n",
    "\n",
    "num_cols = df.select_dtypes(include=['number'])\n",
    "Q1 = num_cols.quantile(0.25)\n",
    "Q3 = num_cols.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "((num_cols < (Q1 - 1.5 * IQR)) | (num_cols > (Q3 + 1.5 * IQR))).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From above output, we know that we have 68 outliers out of total 1495 in annual_income_joint which is 4.5%. We replace it with the median value given the time constraint\n",
    "\n",
    "print(df['annual_income_joint'].quantile(0.50)) #113000\n",
    "print(df['annual_income_joint'].quantile(0.99)) #383417.84\n",
    "df['annual_income_joint'].loc[(df['annual_income_joint'] > 383417.84)] = 113000.0\n",
    "px.box(x=df.annual_income_joint)\n",
    "\n",
    "#Hence, we have dealt with outliers for annual_income_joint. Now we can replace the missing values with mean\n",
    "df['annual_income_joint'] = df['annual_income_joint'].fillna(df['annual_income_joint'].mean())\n",
    "px.box(x=df.annual_income_joint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the debt for each row to impute the debt to income joint ratio\n",
    "df.debt = df.debt_to_income*df.annual_income\n",
    "df.debt_to_income_joint = df.debt/df.annual_income_joint\n",
    "df.debt_to_income_joint.round(2)\n",
    "px.box(x=df.debt_to_income_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning outlier for debt to income joint\n",
    "print(df['debt_to_income_joint'].quantile(0.50)) #9.92\n",
    "print(df['debt_to_income_joint'].quantile(0.99)) #39.20\n",
    "df['debt_to_income_joint'].loc[(df['debt_to_income_joint'] > 39.20)] = 9.92\n",
    "px.box(x=df.debt_to_income_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After we have accounted for missing values, we replace outliers of all numerical columns to the 5th quantile or 95th quantile\n",
    "num_col = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "df[num_col] = df[num_col].apply(lambda x: x.clip(*x.quantile([0.05, 0.95])))\n",
    "\n",
    "#Since we have few NA values in debt to income columns, we drop them\n",
    "df.dropna(subset=['debt_to_income', 'debt_to_income_joint'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Data sampling into training and testing set\n",
    "\n",
    "df.drop(columns=['months_since_last_delinq_categories', 'months_since_last_credit_inquiry', 'account_never_delinq_percent_categories'],inplace=True) #Created for plots, months_since_last_credit_inquiry is redundant/less useful variable\n",
    "\n",
    "modelingDf = df\n",
    "\n",
    "for col_name in modelingDf.columns:\n",
    "    if(modelingDf[col_name].dtype == 'object'):\n",
    "        modelingDf[col_name]= modelingDf[col_name].astype('category')\n",
    "        modelingDf[col_name] = modelingDf[col_name].cat.codes\n",
    "\n",
    "df_train = modelingDf.sample(frac=0.7)\n",
    "df_test = modelingDf.drop(index=df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Creating feature sets\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#I use the step-wise selection wrapper method for feature selection\n",
    "\n",
    "\n",
    "#Before feature selection, drop the highly correlated columns from training set and testing set\n",
    "correlated_features = set()\n",
    "correlation_matrix = modelingDf.corr()\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "df_train.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "df_test.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "df_train_X = df_train.drop('interest_rate', 1)\n",
    "df_train_Y = df_train['interest_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sffs = SFS(LinearRegression(),\n",
    "         k_features=(3,15),\n",
    "         forward=True,\n",
    "         floating=True,\n",
    "         cv=4)\n",
    "features = sffs.fit(df_train_X, df_train_Y)\n",
    "sffs.k_feature_names_\n",
    "\n",
    "filtered_features = df_train_X.columns[list(features.k_feature_idx_)]\n",
    "filtered_features\n",
    "\n",
    "#We have our 15 most impactful features selected hence we can move on to model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Model Selection\n",
    "\n",
    "#Algorithm 1: Best Subset Selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_linear_reg(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    model_k = LinearRegression(fit_intercept = True)\n",
    "    model_k.fit(X,Y)\n",
    "    RSS = mean_squared_error(Y,model_k.predict(X)) * len(Y)\n",
    "    R_squared = model_k.score(X,Y)\n",
    "    return RSS, R_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "import itertools\n",
    "\n",
    "#Initialization variables\n",
    "df_train_X_featured = df_train_X[filtered_features]\n",
    "k = df_train_X_featured.shape[0]\n",
    "RSS_list, R_squared_list, feature_list = [],[], []\n",
    "numb_features = []\n",
    "\n",
    "#Looping over k = 1 to k = 11 features in X\n",
    "for k in tnrange(1,len(df_train_X_featured.columns) + 1, desc = 'Loop...'):\n",
    "\n",
    "    #Looping over all possible combinations: from 11 choose k\n",
    "    for combo in itertools.combinations(df_train_X_featured.columns,k):\n",
    "        tmp_result = fit_linear_reg(df_train_X_featured[list(combo)],df_train_Y)   #Store temp result \n",
    "        RSS_list.append(tmp_result[0])                  #Append lists\n",
    "        R_squared_list.append(tmp_result[1])\n",
    "        feature_list.append(combo)\n",
    "        numb_features.append(len(combo))   \n",
    "\n",
    "#Store in DataFrame\n",
    "modelResult1 = pd.DataFrame({'numb_features': numb_features,'RSS': RSS_list, 'R_squared':R_squared_list,'features':feature_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelResult1['min_RSS'] = modelResult1.groupby('numb_features')['RSS'].transform(min)\n",
    "modelResult1['max_R_squared'] = modelResult1.groupby('numb_features')['R_squared'].transform(max)\n",
    "modelResult1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (16,6))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "ax.scatter(modelResult1.numb_features,modelResult1.RSS, alpha = .2, color = 'darkblue' )\n",
    "ax.set_xlabel('# Features')\n",
    "ax.set_ylabel('RSS')\n",
    "ax.set_title('RSS - Best subset selection')\n",
    "ax.plot(modelResult1.numb_features,modelResult1.min_RSS,color = 'r', label = 'Best subset')\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.scatter(modelResult1.numb_features,modelResult1.R_squared, alpha = .2, color = 'darkblue' )\n",
    "ax.plot(modelResult1.numb_features,modelResult1.max_R_squared,color = 'r', label = 'Best subset')\n",
    "ax.set_xlabel('# Features')\n",
    "ax.set_ylabel('R squared')\n",
    "ax.set_title('R_squared - Best subset selection')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2: OLS method - checking p-values and eliminating non-significant variables\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "def get_stats():\n",
    "    results = sm.OLS(df_train_Y, df_train_X_featured).fit()\n",
    "    print(results.summary())\n",
    "get_stats()\n",
    "\n",
    "#By looking over the p-values of our featured variables, it is clear that the forward selection algorithm used for feature selection gave us a highly efficient model\n",
    "#All variables have p-value << 0 which shows all these variables are highly significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 3: AIC Stepwise Selection model selection - Minimum AIC value model\n",
    "\n",
    "#FOr this purpose, i will be using R afterextracting data from here\n",
    "\n",
    "exportDf = modelingDf[filtered_features]\n",
    "exportDf['interest_rate'] = modelingDf['interest_rate']\n",
    "exportDf.to_csv('r-input-file.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72f51e80378f9f7b0f2e9e0999b0405d317d96da607088ea4163234e834d0ba2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
